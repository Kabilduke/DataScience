## Learning rate:
- The learning rate controls how big a step the model should take to update the weights and biases during training.
## Optimizers:
- An optimizer is an alogrithm to update the weights based on the loss gradient.

#### $1.$ SGD $(Stochastic\ Gradient\ Desent)$
#### $2.$ Adam
- Adaptive Moment Estimation.
#### $3.$ RMSprop
- Adaptive Learning Rate based on magnitude.
#### $4.$ AdaGrad

$Notes$:
- 0.001 Most popular for deep learning
- Adam most commonly used fast, stable and adaptive especially for deep learning.
