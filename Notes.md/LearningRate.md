## Learning rate:
- The learning rate controls how big a step the model should take to update the weights and biases during training.
## Optimizers:
- An optimizer is an alogrithm to update the weights based on the loss gradient.

### Adam
- Adaptive Learning Rate update.
### RMSprop


$Notes$:
- 0.001 Most popular for deep learning
