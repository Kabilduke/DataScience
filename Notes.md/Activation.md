## Linear Activation function 
> Formula:

$$ y = x $$

Notes:
- Output ranges from ($-\infty$ to $\infty$)

## Relu
> Formula:

$$f(x)\ = max(0, x)$$

Notes:
- Values ranges from (0 to $\infty$)

## Sigmoid
> Formula:

$$f(x)\ = {1\over{1+e^{-x}}}$$

Notes:
- Output ranges from 0 and 1
- Used for Binary Classification
- Non Linear activation function 
  
## Softmax

Notes:
- Non Linear activation function
- Mutli Class classification Eg:(LLM for Word next word prediction)
  
## elu
## Tanh
> Formula:

$$f(x) = tanh(x)$$

Note:
- Output values ranges from -1 to +1
